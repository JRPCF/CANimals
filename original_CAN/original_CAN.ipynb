{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Random Seed:  999\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#TODO: bring all parameters (e.g length of z) into 1 cell + restructure code maybe\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.stanford_dogs import StanfordDogs\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "ngpu = 1\n",
    "lr = 0.0001\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = StanfordDogs('./images')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "# (from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original generator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        The following is inspired by \n",
    "        https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "        which seemed a bit clearer and from the CAN paper\n",
    "        '''\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( 100, 2048, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 2048 x 4 x 4\n",
    "            nn.ConvTranspose2d(2048, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 1024 x 8 x 8\n",
    "            nn.ConvTranspose2d( 1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 512 x 16 x 16\n",
    "            nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 256 x 32 x 32\n",
    "            nn.ConvTranspose2d( 256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 128 x 64 x 64\n",
    "            nn.ConvTranspose2d( 128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 64 x 128 x 128\n",
    "            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. 3 x 256 x 256\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original discriminator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu, num_classes=120):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        The following is inspired by \n",
    "        https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "        which seemed a bit clearer and from the CAN paper\n",
    "        '''\n",
    "        \n",
    "        self.ngpu = ngpu\n",
    "        self.num_classes = num_classes\n",
    "        # input is 3 x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, 2, 1, bias=False) \n",
    "        # state size. 32 x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # state size. 64 x 64 x 64\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # state size. 128 x 32 x 32\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        # state size. 256 x 16 x 16\n",
    "        self.conv5 = nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        # state size. 512 x 8 x 8\n",
    "        self.conv6 = nn.Conv2d(512, 512, 4, 2, 1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.real = nn.Linear(512 * 4 * 4, 1)\n",
    "        \n",
    "        self.multi1 = nn.Linear(512 * 4 * 4, 1024)\n",
    "        self.multi2 = nn.Linear(1024, 512)\n",
    "        self.multi3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shared_out = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn2(self.conv2(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn3(self.conv3(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn4(self.conv4(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn5(self.conv5(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn6(self.conv6(shared_out)), 0.2, inplace=True)\n",
    "        \n",
    "\n",
    "        shared_out = shared_out.view(-1, 512 * 4 * 4)\n",
    "\n",
    "        real_output = F.sigmoid(self.real(shared_out))\n",
    "\n",
    "        multi_output = self.multi1(shared_out)\n",
    "        multi_output = self.multi2(multi_output)\n",
    "        multi_output = F.softmax(self.multi3(multi_output))\n",
    "\n",
    "        return real_output, multi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): Tanh()\n",
      "  )\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "G = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    G = nn.DataParallel(G, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "G.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (real): Linear(in_features=8192, out_features=1, bias=True)\n",
      "  (multi1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (multi2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (multi3): Linear(in_features=512, out_features=120, bias=True)\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# D = Discriminator()\n",
    "# Create the Discriminator\n",
    "D = Discriminator(ngpu, num_classes=train_dataset.NUM_CLASSES).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    D = nn.DataParallel(D, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "D.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "# label should be 1 or 0\n",
    "def real_loss(D_out, label=1, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        if label == 1:\n",
    "            labels = torch.ones(batch_size)*0.9\n",
    "        else: # label == 0:\n",
    "            labels = torch.ones(batch_size)*0.1\n",
    "    else:\n",
    "        if label == 1:\n",
    "            labels = torch.ones(batch_size)\n",
    "        else: # label == 0:\n",
    "            labels = torch.zeros(batch_size)\n",
    "        \n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCELoss()  # Changed from BCEWithLogitsLoss, because I saw BCEWithLogitsLoss is for if you don't add the sigmoid loss yourself\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "def multi_loss(D_out, labels):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "# Have to make sure to be correct about maximizing or minimizing loss.\n",
    "# I took the negative of what is mentioned on page 9 in the paper in order to create a loss\n",
    "# to be minimized. If I'm correct real_loss can be used as it is right now\n",
    "def entropy_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    K = train_dataset.NUM_CLASSES\n",
    "    loss = torch.zeros(batch_size)\n",
    "    \n",
    "    # softmaxing\n",
    "    # e = torch.exp(D_out)\n",
    "    # s = torch.sum(e, dim=1)\n",
    "    # probabilities = e / s.view(BATCH_SIZE, 1)\n",
    "    \n",
    "    # Just regular normalization\n",
    "    \n",
    "    #probabilities = D_out / torch.sum(D_out, dim=1).view(batch_size, 1)\n",
    "    \n",
    "    #print(probabilities)\n",
    "            \n",
    "    for c in range(K):\n",
    "        # labels = torch.ones(batch_size) * c\n",
    "        # if train_on_gpu:\n",
    "        #     labels = labels.cuda()\n",
    "        \n",
    "        # c_loss = 1/K * torch.log(probabilities[:, c]) + (1 - 1/K) * torch.log(torch.ones(batch_size)-probabilities[:, c])         \n",
    "        c_loss = 1/K * torch.log(D_out[:, c]) + (1 - 1/K) * torch.log(torch.ones(batch_size)-D_out[:, c])         \n",
    "        \n",
    "        loss += c_loss\n",
    "    #print(loss)\n",
    "    return loss.sum() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%       \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor(-inf)\n",
      "tensor(-6.0882)\n",
      "tensor(-5.7833)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "'''\n",
    "test entropy loss\n",
    "''' \n",
    "D_out_min_entropy = torch.zeros(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "for i in range(BATCH_SIZE):\n",
    "    D_out_min_entropy[i][0] = 1\n",
    "D_out_random = torch.rand(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "for i in range(BATCH_SIZE):\n",
    "    D_out_random[i] /= D_out_random[i].sum()\n",
    "\n",
    "D_out_max_entropy = torch.ones(BATCH_SIZE, train_dataset.NUM_CLASSES) / train_dataset.NUM_CLASSES\n",
    "\n",
    "print(entropy_loss(D_out_min_entropy))\n",
    "print(entropy_loss(D_out_random))\n",
    "print(entropy_loss(D_out_max_entropy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "#real_loss_criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_z = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.SGD(D.parameters(), lr=lr)\n",
    "optimizerG = optim.SGD(G.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training on CPU.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Starting Training Loop...\n",
      "DRR Loss: 0.63826156 DRM Loss: 4.787696 DFR Loss: 0.8224715 D Loss: 6.2484293\n",
      "GFR Loss: 0.605342 GFE Loss: -5.7932434 G Loss: 6.3985853\n",
      "G Loss: 6.3985853 D Loss: 6.2484293\n",
      "Epoch [    1/    5] | d_loss: 6.2484 | g_loss: 6.3986\n",
      "DRR Loss: 0.63880175 DRM Loss: 4.787405 DFR Loss: 0.7317619 D Loss: 6.1579685\n",
      "GFR Loss: 0.67845345 GFE Loss: -5.7937756 G Loss: 6.472229\n",
      "G Loss: 6.472229 D Loss: 6.1579685\n",
      "DRR Loss: 0.6348375 DRM Loss: 4.7874665 DFR Loss: 0.715388 D Loss: 6.137692\n",
      "GFR Loss: 0.69718945 GFE Loss: -5.793563 G Loss: 6.490752\n",
      "G Loss: 6.490752 D Loss: 6.137692\n",
      "DRR Loss: 0.5770676 DRM Loss: 4.78766 DFR Loss: 0.79891783 D Loss: 6.1636457\n",
      "GFR Loss: 0.62124264 GFE Loss: -5.793453 G Loss: 6.4146957\n",
      "G Loss: 6.4146957 D Loss: 6.1636457\n",
      "DRR Loss: 0.571613 DRM Loss: 4.787125 DFR Loss: 0.78569525 D Loss: 6.144433\n",
      "GFR Loss: 0.64613783 GFE Loss: -5.79365 G Loss: 6.439788\n",
      "G Loss: 6.439788 D Loss: 6.144433\n",
      "DRR Loss: 0.45184416 DRM Loss: 4.787702 DFR Loss: 0.898545 D Loss: 6.138091\n",
      "GFR Loss: 0.54109174 GFE Loss: -5.7932673 G Loss: 6.334359\n",
      "G Loss: 6.334359 D Loss: 6.138091\n",
      "DRR Loss: 0.41988087 DRM Loss: 4.7877455 DFR Loss: 0.9996859 D Loss: 6.207312\n",
      "GFR Loss: 0.48055103 GFE Loss: -5.793532 G Loss: 6.274083\n",
      "G Loss: 6.274083 D Loss: 6.207312\n",
      "DRR Loss: 0.36288798 DRM Loss: 4.7873716 DFR Loss: 1.0405298 D Loss: 6.190789\n",
      "GFR Loss: 0.45153376 GFE Loss: -5.793366 G Loss: 6.2448997\n",
      "G Loss: 6.2448997 D Loss: 6.190789\n",
      "DRR Loss: 0.31921825 DRM Loss: 4.787489 DFR Loss: 1.2572796 D Loss: 6.363987\n",
      "GFR Loss: 0.3519981 GFE Loss: -5.7934666 G Loss: 6.145465\n",
      "G Loss: 6.145465 D Loss: 6.363987\n",
      "DRR Loss: 0.24625246 DRM Loss: 4.7872295 DFR Loss: 1.2552805 D Loss: 6.2887626\n",
      "GFR Loss: 0.3542938 GFE Loss: -5.7936716 G Loss: 6.1479654\n",
      "G Loss: 6.1479654 D Loss: 6.2887626\n",
      "DRR Loss: 0.21381949 DRM Loss: 4.7875648 DFR Loss: 1.363706 D Loss: 6.3650904\n",
      "GFR Loss: 0.3127472 GFE Loss: -5.79329 G Loss: 6.106037\n",
      "G Loss: 6.106037 D Loss: 6.3650904\n",
      "DRR Loss: 0.21484059 DRM Loss: 4.787547 DFR Loss: 1.3158069 D Loss: 6.3181944\n",
      "GFR Loss: 0.33338052 GFE Loss: -5.7940364 G Loss: 6.127417\n",
      "G Loss: 6.127417 D Loss: 6.3181944\n",
      "DRR Loss: 0.18363577 DRM Loss: 4.787232 DFR Loss: 1.3221952 D Loss: 6.2930627\n",
      "GFR Loss: 0.32966578 GFE Loss: -5.7931886 G Loss: 6.122854\n",
      "G Loss: 6.122854 D Loss: 6.2930627\n",
      "DRR Loss: 0.19023542 DRM Loss: 4.7875686 DFR Loss: 1.3139226 D Loss: 6.291727\n",
      "GFR Loss: 0.3297435 GFE Loss: -5.7936687 G Loss: 6.123412\n",
      "G Loss: 6.123412 D Loss: 6.291727\n",
      "DRR Loss: 0.1754539 DRM Loss: 4.787499 DFR Loss: 1.1327047 D Loss: 6.0956573\n",
      "GFR Loss: 0.40691307 GFE Loss: -5.7930245 G Loss: 6.199938\n",
      "G Loss: 6.199938 D Loss: 6.0956573\n",
      "DRR Loss: 0.1596833 DRM Loss: 4.787428 DFR Loss: 1.1551253 D Loss: 6.1022363\n",
      "GFR Loss: 0.39859048 GFE Loss: -5.7932653 G Loss: 6.191856\n",
      "G Loss: 6.191856 D Loss: 6.1022363\n",
      "DRR Loss: 0.19438569 DRM Loss: 4.787713 DFR Loss: 0.98985887 D Loss: 5.971957\n",
      "GFR Loss: 0.49065349 GFE Loss: -5.792918 G Loss: 6.2835717\n",
      "G Loss: 6.2835717 D Loss: 5.971957\n",
      "DRR Loss: 0.14770937 DRM Loss: 4.787492 DFR Loss: 0.94972354 D Loss: 5.884925\n",
      "GFR Loss: 0.50470895 GFE Loss: -5.79342 G Loss: 6.2981286\n",
      "G Loss: 6.2981286 D Loss: 5.884925\n",
      "DRR Loss: 0.16568127 DRM Loss: 4.78746 DFR Loss: 0.72429204 D Loss: 5.677433\n",
      "GFR Loss: 0.68724316 GFE Loss: -5.7931857 G Loss: 6.4804287\n",
      "G Loss: 6.4804287 D Loss: 5.677433\n",
      "DRR Loss: 0.18186669 DRM Loss: 4.787509 DFR Loss: 0.6571441 D Loss: 5.6265197\n",
      "GFR Loss: 0.76977587 GFE Loss: -5.7934217 G Loss: 6.5631976\n",
      "G Loss: 6.5631976 D Loss: 5.6265197\n",
      "DRR Loss: 0.15874028 DRM Loss: 4.787315 DFR Loss: 0.59816116 D Loss: 5.5442166\n",
      "GFR Loss: 0.8353561 GFE Loss: -5.79298 G Loss: 6.6283364\n",
      "G Loss: 6.6283364 D Loss: 5.5442166\n",
      "DRR Loss: 0.17093414 DRM Loss: 4.7873125 DFR Loss: 0.51650447 D Loss: 5.474751\n",
      "GFR Loss: 0.94100547 GFE Loss: -5.793403 G Loss: 6.7344084\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "num_epochs = 5 #50\n",
    "print_every = 50\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for batch_i, (real_images, real_labels) in enumerate(train_dataloader, 0):\n",
    "        b_size = real_images.size(0)\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        # 3.\n",
    "        #z = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100)) \n",
    "        #z = torch.from_numpy(z).float()\n",
    "        z = torch.randn(b_size, 100, 1, 1, device=device)\n",
    "        #if train_on_gpu:\n",
    "        #    z = z.cuda()\n",
    "        \n",
    "        # 4) Generate fake image batch with G\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            real_images = real_images.cuda()\n",
    "        \n",
    "        # 5) Forward pass real batch through D\n",
    "        D_real, D_multi = D(real_images) #.view(-1)\n",
    "        d_real_real_loss = real_loss(D_real, label=1) \n",
    "        # 6.\n",
    "        d_real_multi_loss = multi_loss(D_multi, real_labels)\n",
    "        # 7.\n",
    "        D_fake, D_fake_entropy = D(fake_images)\n",
    "        d_fake_real_loss = real_loss(D_fake, label=0)\n",
    "        g_fake_real_loss = real_loss(D_fake, label=1)\n",
    "        # 8.\n",
    "        g_fake_entropy_loss = entropy_loss(D_fake_entropy) ##\n",
    "        \n",
    "        # 9.\n",
    "        #d_loss= torch.log(d_real_real_loss)+torch.log(d_real_multi_loss)+torch.log(d_fake_real_loss) \n",
    "        d_loss = d_real_real_loss + d_real_multi_loss + d_fake_real_loss\n",
    "        \n",
    "        #torch.log(1-g_fake_real_loss), the 1- is not necessary because computed against label=0 now\n",
    "        print('DRR Loss:', d_real_real_loss.data.numpy(),\n",
    "              'DRM Loss:', d_real_multi_loss.data.numpy(), \n",
    "              'DFR Loss:',d_fake_real_loss.data.numpy(), \n",
    "              'D Loss:',d_loss.data.numpy())\n",
    "        \n",
    "        # 10.\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # 11.\n",
    "        #g_loss=torch.log(g_fake_real_loss)-g_fake_entropy_loss\n",
    "        g_loss=g_fake_real_loss - g_fake_entropy_loss\n",
    "        print('GFR Loss:',g_fake_real_loss.data.numpy(), \n",
    "              'GFE Loss:',g_fake_entropy_loss.data.numpy(), \n",
    "              'G Loss:',g_loss.data.numpy())\n",
    "        \n",
    "        # 12.\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        print('G Loss:', g_loss.data.numpy(), 'D Loss:', d_loss.data.numpy())\n",
    "\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            G_losses.append(g_loss.item())\n",
    "            D_losses.append(d_loss.item())\n",
    "            \n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    img_z = G(fixed_z).detach().cpu()\n",
    "    img_list.append(make_grid(img_z, padding=2, normalize=True))\n",
    "    G.train() # back to training mode    \n",
    "    \n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(img_list, f)\n",
    "    \n",
    "torch.save(G, 'G.pt')\n",
    "torch.save(D, 'D.pt')\n",
    "\n",
    "\n",
    "### END -   FROM Udacity DCGAN implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}