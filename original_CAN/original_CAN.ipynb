{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.stanford_dogs import StanfordDogs\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6545ddbd7ff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordDogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/SEAS Spring 2020/DL/CANimals/original_CAN/data/stanford_dogs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, specific_classes)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#     ])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecific_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecific_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_dataset_for_use\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/SEAS Spring 2020/DL/CANimals/original_CAN/data/stanford_dogs.py\u001b[0m in \u001b[0;36mload_data_from_path\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# 2 Read in the images, with corresponding label the index of the class name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Taking [10:] makes us get the dog name without a code in the front\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = StanfordDogs('./images')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper method to create convolutional layers\n",
    "def conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper method to create deconvolutional layers\n",
    "def deconv(in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "    return nn.ConvTranspose2d(in_channels, out_channels,kernel_size,stride,padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original discriminator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        the original can architecture has been substituted with\n",
    "        one inspired by the Udacity DCGAN\n",
    "        \"\"\"\n",
    "        self.conv1 = conv(3, 32)\n",
    "        self.conv2 = conv(32, 32*2)\n",
    "        self.conv3 = conv(32*2, 32*4)\n",
    "        \n",
    "        self.multi = nn.Linear(32*4*4*4, num_classes)\n",
    "        \n",
    "        self.real = nn.Linear(32*4*4*4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        out = F.leaky_relu(self.conv2(out), 0.2)\n",
    "        out = F.leaky_relu(self.conv3(out), 0.2)\n",
    "        \n",
    "        out = out.view(-1, 32*4*4*4)\n",
    "        \n",
    "        real_output = F.sigmoid(self.real(out))\n",
    "        \n",
    "        multi_output = F.softmax(self.multi(out))\n",
    "        \n",
    "        return real_output, multi_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original generator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        the original can architecture has been substituted with\n",
    "        one inspired by the Udacity DCGAN\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc = nn.Linear(100, 32*4*4*4)\n",
    "        self.t_conv1 = deconv(32*4, 32*2)\n",
    "        self.t_conv2 = deconv(32*2, 32)\n",
    "        self.t_conv3 = deconv(32, 3)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = out.view(-1, 32*4, 4, 4) # (batch_size, depth, 4, 4)\n",
    "        out = F.relu(self.t_conv1(out))\n",
    "        out = F.relu(self.t_conv2(out))\n",
    "        out = self.t_conv3(out)\n",
    "        out = F.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "def real_loss(D_out, smooth=False):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(BATCH_SIZE)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(BATCH_SIZE) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "def multi_loss(D_out, labels):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b1b510928456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m ''' \n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mD_out_min_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mD_out_min_entropy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "# Have to make sure to be correct about maximizing or minimizing loss.\n",
    "# I took the negative of what is mentioned on page 9 in the paper in order to create a loss\n",
    "# to be minimized. If I'm correct real_loss can be used as it is right now\n",
    "def entropy_loss(D_out):\n",
    "    ### TO BE COMPLETED\n",
    "    K = train_dataset.NUM_CLASSES\n",
    "    loss = torch.zeros(BATCH_SIZE)\n",
    "    \n",
    "    # softmaxing\n",
    "    # e = torch.exp(D_out)\n",
    "    # s = torch.sum(e, dim=1)\n",
    "    # probabilities = e / s.view(BATCH_SIZE, 1)\n",
    "    \n",
    "    # Just regular normalization\n",
    "    probabilities = D_out / torch.sum(D_out, dim=1).view(BATCH_SIZE, 1)\n",
    "    \n",
    "    print(probabilities)\n",
    "            \n",
    "    for c in range(K):\n",
    "        # labels = torch.ones(batch_size) * c\n",
    "        # if train_on_gpu:\n",
    "        #     labels = labels.cuda()\n",
    "        \n",
    "        c_loss = - (1/K * torch.log(probabilities[:, c]) + (1 - 1/K) * torch.log(torch.ones(BATCH_SIZE)-probabilities[:, c]))         \n",
    "        loss += c_loss\n",
    "    return loss\n",
    "        \n",
    "'''\n",
    "test\n",
    "''' \n",
    "D_out_min_entropy = torch.zeros(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "for i in range(BATCH_SIZE):\n",
    "    D_out_min_entropy[i][0] = 1\n",
    "D_out_random = torch.rand(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "\n",
    "D_out_max_entropy = torch.ones(BATCH_SIZE, train_dataset.NUM_CLASSES) \n",
    "\n",
    "print(entropy_loss(D_out_min_entropy))\n",
    "print(entropy_loss(D_out_random))\n",
    "print(entropy_loss(D_out_max_entropy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "num_epochs = 50\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "print_every = 300\n",
    "sample_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4e93ba02f8f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# batch_size = real_images.size(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "# to collect samples from the generator\n",
    "fixed_z = torch.from_numpy(np.random.uniform(-1, 1, size=(sample_size, 100))).float()\n",
    "\n",
    "# 2.\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, real_labels) in enumerate(train_dataloader):\n",
    "                \n",
    "        # batch_size = real_images.size(0)\n",
    "        \n",
    "        # important rescaling step\n",
    "        #real_images = scale(real_images)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # 3.\n",
    "        z = np.random.uniform(-1, 1, size=(BATCH_SIZE, z_size)) \n",
    "        z = torch.from_numpy(z).float()\n",
    "        if train_on_gpu:\n",
    "            z = z.cuda()\n",
    "            \n",
    "        # 4.\n",
    "        fake_images = G(z)\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            real_images = real_images.cuda()\n",
    "        \n",
    "        # 5.\n",
    "        D_real, D_multi = D(real_images)\n",
    "        d_real_real_loss = real_loss(D_real) \n",
    "        # 6.\n",
    "        d_real_multi_loss = multi_loss(D_multi, real_labels)\n",
    "        # 7.\n",
    "        D_fake = D(fake_images)\n",
    "        d_fake_real_loss = real_loss(D_fake)\n",
    "        # 8.\n",
    "        g_fake_entropy_loss = entropy_loss(D_fake) ##\n",
    "        \n",
    "        # 9.\n",
    "        d_loss= log(d_real_real_loss)+log(d_real_multi_loss)+log(1-d_fake_real_loss)\n",
    "        \n",
    "        # 10.\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 11.\n",
    "        g_loss=log(d_fake_real_loss)-g_fake_entropy_loss\n",
    "        \n",
    "        # 12.\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        \n",
    "### START - FROM Udacity DCGAN implementation ###\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            losses.append((d_loss.item(), g_loss.item()))\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    samples_z = G(fixed_z)\n",
    "    samples.append(samples_z)\n",
    "    G.train() # back to training mode\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)\n",
    "    \n",
    "### END -   FROM Udacity DCGAN implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
