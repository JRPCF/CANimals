{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Random Seed:  999\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#TODO: bring all parameters (e.g length of z) into 1 cell + restructure code maybe\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.stanford_dogs import StanfordDogs\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "ngpu = 1\n",
    "lr = 0.0001\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = StanfordDogs('./images')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "# (from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original generator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # \"\"\"\n",
    "        # the original can architecture has been substituted with\n",
    "        # one inspired by the Udacity DCGAN\n",
    "        # \"\"\"\n",
    "        # self.fc = nn.Linear(100, 32*4*4*4)\n",
    "        # self.t_conv1 = deconv(32*4, 32*2)\n",
    "        # self.t_conv2 = deconv(32*2, 32)\n",
    "        # self.t_conv3 = deconv(32, 3)\n",
    "        \n",
    "        '''\n",
    "        The following is inspired by \n",
    "        https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "        which seemed a bit clearer and from the CAN paper\n",
    "        '''\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( 100, 2048, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 2048 x 4 x 4\n",
    "            nn.ConvTranspose2d(2048, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 1024 x 8 x 8\n",
    "            nn.ConvTranspose2d( 1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 512 x 16 x 16\n",
    "            nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 256 x 32 x 32\n",
    "            nn.ConvTranspose2d( 256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 128 x 64 x 64\n",
    "            nn.ConvTranspose2d( 128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 64 x 128 x 128\n",
    "            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. 3 x 256 x 256\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # out = self.fc(x)\n",
    "        # out = out.view(-1, 32*4, 4, 4) # (batch_size, depth, 4, 4)\n",
    "        # out = F.relu(self.t_conv1(out))\n",
    "        # out = F.relu(self.t_conv2(out))\n",
    "        # out = self.t_conv3(out)\n",
    "        # out = F.tanh(out)\n",
    "        # return out\n",
    "        return self.main(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original discriminator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu, num_classes=120):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # \"\"\"\n",
    "        # the original can architecture has been substituted with\n",
    "        # one inspired by the Udacity DCGAN\n",
    "        # \"\"\"\n",
    "        # self.conv1 = conv(3, 32)\n",
    "        # self.conv2 = conv(32, 32*2)\n",
    "        # self.conv3 = conv(32*2, 32*4)\n",
    "        # \n",
    "        # self.multi = nn.Linear(32*4*4*4, num_classes)\n",
    "        # \n",
    "        # self.real = nn.Linear(32*4*4*4, 1)\n",
    "        self.ngpu = ngpu\n",
    "        self.num_classes = num_classes\n",
    "        # input is 3 x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, 2, 1, bias=False) \n",
    "        # state size. 32 x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # state size. 64 x 64 x 64\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # state size. 128 x 32 x 32\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        # state size. 256 x 16 x 16\n",
    "        self.conv5 = nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        # state size. 512 x 8 x 8\n",
    "        self.conv6 = nn.Conv2d(512, 512, 4, 2, 1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.real = nn.Linear(512 * 4 * 4, 1)\n",
    "        \n",
    "        self.multi1 = nn.Linear(512 * 4 * 4, 1024)\n",
    "        self.multi2 = nn.Linear(1024, 512)\n",
    "        self.multi3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # self.main = nn.Sequential(\n",
    "        #     # input is 3 x 256 x 256\n",
    "        #     nn.Conv2d(3, 32, 4, 2, 1, bias=False),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     # state size. 32 x 128 x 128\n",
    "        #     nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     # state size. 64 x 64 x 64\n",
    "        #     nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(128),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     # state size. 128 x 32 x 32\n",
    "        #     nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(256),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     # state size. 256 x 16 x 16\n",
    "        #     nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     # state size. 512 x 8 x 8\n",
    "        #     nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n",
    "        #     nn.BatchNorm2d(512),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "        #     # state size. 512 x 4 x 4\n",
    "        # )\n",
    "        # \n",
    "        # self.real_fake = nn.Sequential(\n",
    "        #     nn.Linear(512 * 4 * 4, 1),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "        # \n",
    "        # self.multi_class = nn.Sequential(\n",
    "        #     nn.Linear(512 * 4 * 4, 1024),\n",
    "        #     nn.Linear(1024, 512),\n",
    "        #     nn.Linear(512, num_classes),\n",
    "        #     nn.Softmax()\n",
    "        #     \n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn2(self.conv2(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn3(self.conv3(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn4(self.conv4(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn5(self.conv5(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn6(self.conv6(shared_out)), 0.2, inplace=True)\n",
    "        \n",
    "\n",
    "        shared_out = shared_out.view(-1, 512 * 4 * 4)\n",
    "\n",
    "        real_output = F.sigmoid(self.real(shared_out))\n",
    "\n",
    "        multi_output = self.multi1(shared_out)\n",
    "        multi_output = self.multi2(multi_output)\n",
    "        multi_output = F.softmax(self.multi3(multi_output))\n",
    "\n",
    "        return real_output, multi_output\n",
    "        \n",
    "        # \n",
    "        # shared = self.main(x)\n",
    "        # real_output = self.real_fake(shared)\n",
    "        # multi_output = self.multi_class(shared)\n",
    "        # return real_output, multi_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): Tanh()\n",
      "  )\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "G = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    G = nn.DataParallel(G, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "G.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(G)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (real): Linear(in_features=8192, out_features=1, bias=True)\n",
      "  (multi1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (multi2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (multi3): Linear(in_features=512, out_features=120, bias=True)\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# D = Discriminator()\n",
    "# Create the Discriminator\n",
    "D = Discriminator(ngpu, num_classes=train_dataset.NUM_CLASSES).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    D = nn.DataParallel(D, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "D.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "# label should be 1 or 0\n",
    "def real_loss(D_out, label=1, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        if label == 1:\n",
    "            labels = torch.ones(batch_size)*0.9\n",
    "        else: # label == 0:\n",
    "            labels = torch.ones(batch_size)*0.1\n",
    "    else:\n",
    "        if label == 1:\n",
    "            labels = torch.ones(batch_size)\n",
    "        else: # label == 0:\n",
    "            labels = torch.zeros(batch_size)\n",
    "        \n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCELoss()  # Changed from BCEWithLogitsLoss, because I saw BCEWithLogitsLoss is for if you don't add the sigmoid loss yourself\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "def multi_loss(D_out, labels):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "# Have to make sure to be correct about maximizing or minimizing loss.\n",
    "# I took the negative of what is mentioned on page 9 in the paper in order to create a loss\n",
    "# to be minimized. If I'm correct real_loss can be used as it is right now\n",
    "def entropy_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    K = train_dataset.NUM_CLASSES\n",
    "    loss = torch.zeros(batch_size)\n",
    "    \n",
    "    # softmaxing\n",
    "    # e = torch.exp(D_out)\n",
    "    # s = torch.sum(e, dim=1)\n",
    "    # probabilities = e / s.view(BATCH_SIZE, 1)\n",
    "    \n",
    "    # Just regular normalization\n",
    "    \n",
    "    #probabilities = D_out / torch.sum(D_out, dim=1).view(batch_size, 1)\n",
    "    \n",
    "    #print(probabilities)\n",
    "            \n",
    "    for c in range(K):\n",
    "        # labels = torch.ones(batch_size) * c\n",
    "        # if train_on_gpu:\n",
    "        #     labels = labels.cuda()\n",
    "        \n",
    "        # c_loss = 1/K * torch.log(probabilities[:, c]) + (1 - 1/K) * torch.log(torch.ones(batch_size)-probabilities[:, c])         \n",
    "        c_loss = 1/K * torch.log(D_out[:, c]) + (1 - 1/K) * torch.log(torch.ones(batch_size)-D_out[:, c])         \n",
    "        \n",
    "        loss += c_loss\n",
    "    #print(loss)\n",
    "    return loss.sum() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor(inf)\n",
      "tensor(779.5074)\n",
      "tensor(740.2642)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "'''\n",
    "test entropy loss\n",
    "''' \n",
    "D_out_min_entropy = torch.zeros(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "for i in range(BATCH_SIZE):\n",
    "    D_out_min_entropy[i][0] = 1\n",
    "D_out_random = torch.rand(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "\n",
    "D_out_max_entropy = torch.ones(BATCH_SIZE, train_dataset.NUM_CLASSES) \n",
    "\n",
    "print(entropy_loss(D_out_min_entropy))\n",
    "print(entropy_loss(D_out_random))\n",
    "print(entropy_loss(D_out_max_entropy))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%       \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "#real_loss_criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_z = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.SGD(D.parameters(), lr=lr)\n",
    "optimizerG = optim.SGD(G.parameters(), lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training on CPU.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Starting Training Loop...\n",
      "tensor(0.6293, grad_fn=<BinaryCrossEntropyBackward>) tensor(4.7875, grad_fn=<NllLossBackward>) tensor(0.6247, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6323, grad_fn=<AddBackward0>)\n",
      "tensor(0.8056, grad_fn=<BinaryCrossEntropyBackward>) tensor(-5.7932, grad_fn=<DivBackward0>) tensor(5.5770, grad_fn=<SubBackward0>)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:106: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ade5b712744a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# 12.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ],
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error"
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "num_epochs = 5 #50\n",
    "print_every = 50\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for batch_i, (real_images, real_labels) in enumerate(train_dataloader, 0):\n",
    "        b_size = real_images.size(0)\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        # 3.\n",
    "        #z = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100)) \n",
    "        #z = torch.from_numpy(z).float()\n",
    "        z = torch.randn(b_size, 100, 1, 1, device=device)\n",
    "        #if train_on_gpu:\n",
    "        #    z = z.cuda()\n",
    "        \n",
    "        # 4) Generate fake image batch with G\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            real_images = real_images.cuda()\n",
    "        \n",
    "        # 5) Forward pass real batch through D\n",
    "        D_real, D_multi = D(real_images) #.view(-1)\n",
    "        d_real_real_loss = real_loss(D_real, label=1) \n",
    "        # 6.\n",
    "        d_real_multi_loss = multi_loss(D_multi, real_labels)\n",
    "        # 7.\n",
    "        D_fake, D_fake_entropy = D(fake_images)\n",
    "        d_fake_real_loss = real_loss(D_fake, label=0)\n",
    "        g_fake_real_loss = real_loss(D_fake, label=1)\n",
    "        # 8.\n",
    "        g_fake_entropy_loss = entropy_loss(D_fake_entropy) ##\n",
    "        \n",
    "        # 9.\n",
    "        d_loss= torch.log(d_real_real_loss)+torch.log(d_real_multi_loss)+torch.log(d_fake_real_loss)  #torch.log(1-g_fake_real_loss), the 1- is not necessary because computed against label=0 now\n",
    "        print(d_real_real_loss, d_real_multi_loss, d_fake_real_loss, d_loss)\n",
    "        \n",
    "        # 10.\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # 11.\n",
    "        \n",
    "        g_loss=torch.log(g_fake_real_loss)-g_fake_entropy_loss\n",
    "        print(g_fake_real_loss, g_fake_entropy_loss, g_loss)\n",
    "        \n",
    "        # 12.\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            G_losses.append(g_loss.item())\n",
    "            D_losses.append(d_loss.item())\n",
    "            \n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    img_z = G(fixed_z).detach().cpu()\n",
    "    img_list.append(make_grid(img_z, padding=2, normalize=True))\n",
    "    G.train() # back to training mode    \n",
    "    \n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(img_list, f)\n",
    "    \n",
    "torch.save(G, 'G.pt')\n",
    "torch.save(D, 'D.pt')\n",
    "\n",
    "\n",
    "### END -   FROM Udacity DCGAN implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}