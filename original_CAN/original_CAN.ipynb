{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.stanford_dogs import StanfordDogs\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = StanfordDogs('./images')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original discriminator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        v=256*256*3\n",
    "        n=int(((v-4*4+2)/2+1)*32) #((input_volumeâˆ’kernel_volume+2padding)/stride+1)*numberOfLayers\n",
    "        \n",
    "        self.conv1 = self.conv(v, n, 4, 2,1) #(32 4x4)\n",
    "        self.conv2 = self.conv(n,2*n) #(64 4x4)\n",
    "        self.conv3 = self.conv(2*n, 4*n) #(128 4x4)\n",
    "        self.conv4 = self.conv(4*n, 8*n) #(256 4x4)\n",
    "        self.conv5 = self.conv(8*n, 16*n) #(512 4x4)\n",
    "        self.conv6 = self.conv(16*n, 16*n) #(512 4x4)\n",
    "        \n",
    "        self.multi1 = nn.Linear(16*n, 1024)\n",
    "        self.multi2 = nn.Linear(1024, 512)\n",
    "        self.multi3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.real = nn.Linear(16*n, 2)\n",
    "\n",
    "    #helper method to create convolutional layers\n",
    "    def conv(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.leaky_relu((self.conv1(x)), 0.1)\n",
    "        out = F.leaky_relu((self.conv2(out)), 0.1)\n",
    "        out = F.leaky_relu((self.conv3(out)), 0.1)\n",
    "        out = F.leaky_relu((self.conv4(out)), 0.1)\n",
    "        out = F.leaky_relu((self.conv5(out)), 0.1)\n",
    "        out = F.leaky_relu((self.conv6(out)), 0.1)\n",
    "        \n",
    "        out.flatten()\n",
    "        \n",
    "        multi_output = self.multi1(out)\n",
    "        multi_output = self.multi2(multi_output)\n",
    "        multi_output = self.multi3(multi_output)\n",
    "        multi_output = F.softmax(multi_output)\n",
    "        \n",
    "        real_output = self.real(out)\n",
    "        real_output = self.sigmoid(real_output)\n",
    "        \n",
    "        return real_output, multi_output\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original generator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        conv_dim = 256*256*3\n",
    "        \n",
    "        self.fc = nn.Linear(100, conv_dim*2*2*2*2*2*2*2)\n",
    "        \n",
    "        self.t_conv1 = self.deconv(conv_dim*2*2*2*2*2*2*2, conv_dim*2*2*2*2*2*2)\n",
    "        self.t_conv2 = self.deconv(conv_dim*2*2*2*2*2*2, conv_dim*2*2*2*2*2)\n",
    "        self.t_conv3 = self.deconv(conv_dim*2*2*2*2*2, conv_dim*2*2*2*2)\n",
    "        self.t_conv4 = self.deconv(conv_dim*2*2*2*2, conv_dim*2*2*2)\n",
    "        self.t_conv5 = self.deconv(conv_dim*2*2*2, conv_dim*2*2)\n",
    "        self.t_conv6 = self.deconv(conv_dim*2*2, conv_dim*2)\n",
    "        self.t_conv7 = self.deconv(conv_dim*2, conv_dim)\n",
    "        \n",
    "    #helper method to create deconvolutional layers\n",
    "    def deconv(self,in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels,kernel_size,stride,padding, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = out.view(-1, 2048) \n",
    "        \n",
    "        out = F.leaky_relu(t_conv1(out),0.1)\n",
    "        out = F.leaky_relu(t_conv2(out),0.1)\n",
    "        out = F.leaky_relu(t_conv3(out),0.1)\n",
    "        out = F.leaky_relu(t_conv4(out),0.1)\n",
    "        out = F.leaky_relu(t_conv5(out),0.1)\n",
    "        out = F.leaky_relu(t_conv6(out),0.1)\n",
    "        out = F.leaky_relu(t_conv7(out),0.1)\n",
    "        \n",
    "        out = F.tanh(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "def real_loss(D_out, smooth=False):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(BATCH_SIZE)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(BATCH_SIZE) # real labels = 1\n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "def multi_loss(D_out, labels):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
      "        inf, inf, inf, inf, inf, inf, inf, inf])\n",
      "tensor([[0.0057, 0.0159, 0.0150,  ..., 0.0003, 0.0152, 0.0035],\n",
      "        [0.0125, 0.0026, 0.0011,  ..., 0.0057, 0.0005, 0.0067],\n",
      "        [0.0093, 0.0155, 0.0134,  ..., 0.0081, 0.0097, 0.0117],\n",
      "        ...,\n",
      "        [0.0142, 0.0067, 0.0149,  ..., 0.0019, 0.0154, 0.0153],\n",
      "        [0.0085, 0.0061, 0.0090,  ..., 0.0034, 0.0124, 0.0022],\n",
      "        [0.0013, 0.0118, 0.0033,  ..., 0.0084, 0.0045, 0.0152]])\n",
      "tensor([6.0636, 6.0934, 6.1159, 5.9970, 6.0932, 6.1287, 6.0779, 6.1407, 6.0515,\n",
      "        6.0578, 6.0718, 6.0623, 6.1187, 6.1434, 6.1215, 6.1283, 6.2140, 6.0174,\n",
      "        6.1290, 6.0463, 6.1232, 6.0707, 6.1602, 6.0756, 6.0330, 6.1776, 6.0977,\n",
      "        6.0858, 6.0915, 6.0663, 6.0439, 6.0632])\n",
      "tensor([[0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083],\n",
      "        [0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083],\n",
      "        [0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083],\n",
      "        ...,\n",
      "        [0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083],\n",
      "        [0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083],\n",
      "        [0.0083, 0.0083, 0.0083,  ..., 0.0083, 0.0083, 0.0083]])\n",
      "tensor([5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833,\n",
      "        5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833,\n",
      "        5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833, 5.7833,\n",
      "        5.7833, 5.7833, 5.7833, 5.7833, 5.7833])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "# Have to make sure to be correct about maximizing or minimizing loss.\n",
    "# I took the negative of what is mentioned on page 9 in the paper in order to create a loss\n",
    "# to be minimized. If I'm correct real_loss can be used as it is right now\n",
    "def entropy_loss(D_out):\n",
    "    ### TO BE COMPLETED\n",
    "    K = train_dataset.NUM_CLASSES\n",
    "    loss = torch.zeros(BATCH_SIZE)\n",
    "    \n",
    "    # softmaxing\n",
    "    # e = torch.exp(D_out)\n",
    "    # s = torch.sum(e, dim=1)\n",
    "    # probabilities = e / s.view(BATCH_SIZE, 1)\n",
    "    \n",
    "    # Just regular normalization\n",
    "    probabilities = D_out / torch.sum(D_out, dim=1).view(BATCH_SIZE, 1)\n",
    "    \n",
    "    print(probabilities)\n",
    "            \n",
    "    for c in range(K):\n",
    "        # labels = torch.ones(batch_size) * c\n",
    "        # if train_on_gpu:\n",
    "        #     labels = labels.cuda()\n",
    "        \n",
    "        c_loss = - (1/K * torch.log(probabilities[:, c]) + (1 - 1/K) * torch.log(torch.ones(BATCH_SIZE)-probabilities[:, c]))         \n",
    "        loss += c_loss\n",
    "    return loss\n",
    "        \n",
    "'''\n",
    "test\n",
    "''' \n",
    "D_out_min_entropy = torch.zeros(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "for i in range(BATCH_SIZE):\n",
    "    D_out_min_entropy[i][0] = 1\n",
    "D_out_random = torch.rand(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "\n",
    "D_out_max_entropy = torch.ones(BATCH_SIZE, train_dataset.NUM_CLASSES) \n",
    "\n",
    "print(entropy_loss(D_out_min_entropy))\n",
    "print(entropy_loss(D_out_random))\n",
    "print(entropy_loss(D_out_max_entropy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "num_epochs = 50\n",
    "\n",
    "# keep track of loss and generated, \"fake\" samples\n",
    "samples = []\n",
    "losses = []\n",
    "print_every = 300\n",
    "sample_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "# to collect samples from the generator\n",
    "fixed_z = torch.from_numpy(np.random.uniform(-1, 1, size=(sample_size, 100))).float()\n",
    "\n",
    "# 2.\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for batch_i, (real_images, real_labels) in enumerate(train_dataloader):\n",
    "                \n",
    "        # batch_size = real_images.size(0)\n",
    "        \n",
    "        # important rescaling step\n",
    "        #real_images = scale(real_images)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # 3.\n",
    "        z = np.random.uniform(-1, 1, size=(BATCH_SIZE, z_size)) \n",
    "        z = torch.from_numpy(z).float()\n",
    "        if train_on_gpu:\n",
    "            z = z.cuda()\n",
    "            \n",
    "        # 4.\n",
    "        fake_images = G(z)\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            real_images = real_images.cuda()\n",
    "        \n",
    "        # 5.\n",
    "        D_real, D_multi = D(real_images)\n",
    "        d_real_real_loss = real_loss(D_real) \n",
    "        # 6.\n",
    "        d_real_multi_loss = multi_loss(D_multi, real_labels)\n",
    "        # 7.\n",
    "        D_fake = D(fake_images)\n",
    "        d_fake_real_loss = real_loss(D_fake)\n",
    "        # 8.\n",
    "        g_fake_entropy_loss = entropy_loss(D_fake) ##\n",
    "        \n",
    "        # 9.\n",
    "        d_loss= log(d_real_real_loss)+log(d_real_multi_loss)+log(1-d_fake_real_loss)\n",
    "        \n",
    "        # 10.\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 11.\n",
    "        g_loss=log(d_fake_real_loss)-g_fake_entropy_loss\n",
    "        \n",
    "        # 12.\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        \n",
    "### START - FROM Udacity DCGAN implementation ###\n",
    "        # Print some loss stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            losses.append((d_loss.item(), g_loss.item()))\n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    samples_z = G(fixed_z)\n",
    "    samples.append(samples_z)\n",
    "    G.train() # back to training mode\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)\n",
    "    \n",
    "### END -   FROM Udacity DCGAN implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}