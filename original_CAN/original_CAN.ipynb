{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#TODO: bring all parameters (e.g length of z) into 1 cell + restructure code maybe\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.stanford_dogs import StanfordDogs\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "ngpu = 1\n",
    "lr = 0.0001\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = StanfordDogs('./images')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "# (from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original generator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        The following is inspired by \n",
    "        https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "        which seemed a bit clearer and from the CAN paper\n",
    "        '''\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( 100, 2048, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 2048 x 4 x 4\n",
    "            nn.ConvTranspose2d(2048, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 1024 x 8 x 8\n",
    "            nn.ConvTranspose2d( 1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 512 x 16 x 16\n",
    "            nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 256 x 32 x 32\n",
    "            nn.ConvTranspose2d( 256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 128 x 64 x 64\n",
    "            nn.ConvTranspose2d( 128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # state size. 64 x 128 x 128\n",
    "            nn.ConvTranspose2d( 64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. 3 x 256 x 256\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of original discriminator\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu, num_classes=120):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        The following is inspired by \n",
    "        https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "        which seemed a bit clearer and from the CAN paper\n",
    "        '''\n",
    "        \n",
    "        self.ngpu = ngpu\n",
    "        self.num_classes = num_classes\n",
    "        # input is 3 x 256 x 256\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, 2, 1, bias=False) \n",
    "        # state size. 32 x 128 x 128\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # state size. 64 x 64 x 64\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # state size. 128 x 32 x 32\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        # state size. 256 x 16 x 16\n",
    "        self.conv5 = nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        # state size. 512 x 8 x 8\n",
    "        self.conv6 = nn.Conv2d(512, 512, 4, 2, 1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.real = nn.Linear(512 * 4 * 4, 1)\n",
    "        \n",
    "        self.multi1 = nn.Linear(512 * 4 * 4, 1024)\n",
    "        self.multi2 = nn.Linear(1024, 512)\n",
    "        self.multi3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shared_out = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn2(self.conv2(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn3(self.conv3(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn4(self.conv4(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn5(self.conv5(shared_out)), 0.2, inplace=True)\n",
    "        shared_out = F.leaky_relu(self.bn6(self.conv6(shared_out)), 0.2, inplace=True)\n",
    "        \n",
    "\n",
    "        shared_out = shared_out.view(-1, 512 * 4 * 4)\n",
    "\n",
    "        real_output = F.sigmoid(self.real(shared_out))\n",
    "\n",
    "        multi_output = self.multi1(shared_out)\n",
    "        multi_output = self.multi2(multi_output)\n",
    "        multi_output = F.softmax(self.multi3(multi_output))\n",
    "\n",
    "        return real_output, multi_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace)\n",
      "    (15): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace)\n",
      "    (18): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "G = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    G = nn.DataParallel(G, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "G.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (real): Linear(in_features=8192, out_features=1, bias=True)\n",
      "  (multi1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (multi2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (multi3): Linear(in_features=512, out_features=120, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# D = Discriminator()\n",
    "# Create the Discriminator\n",
    "D = Discriminator(ngpu, num_classes=train_dataset.NUM_CLASSES).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    D = nn.DataParallel(D, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "D.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "# label should be 1 or 0\n",
    "def real_loss(D_out, label=1, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        if label == 1:\n",
    "            labels = torch.ones(batch_size)*0.9\n",
    "        else: # label == 0:\n",
    "            labels = torch.ones(batch_size)*0.1\n",
    "    else:\n",
    "        if label == 1:\n",
    "            labels = torch.ones(batch_size)\n",
    "        else: # label == 0:\n",
    "            labels = torch.zeros(batch_size)\n",
    "        \n",
    "    # move labels to GPU if available     \n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    # binary cross entropy with logits loss\n",
    "    criterion = nn.BCELoss()  # Changed from BCEWithLogitsLoss, because I saw BCEWithLogitsLoss is for if you don't add the sigmoid loss yourself\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODIFIED Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "J.R. Carneiro JC4896\n",
    "\"\"\"\n",
    "def multi_loss(D_out, labels):\n",
    "    # batch_size = D_out.size(0)\n",
    "    # labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    if train_on_gpu:\n",
    "        labels = labels.cuda()\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yarne Hermann YPH2105\n",
    "\"\"\"\n",
    "# Have to make sure to be correct about maximizing or minimizing loss.\n",
    "# I took the negative of what is mentioned on page 9 in the paper in order to create a loss\n",
    "# to be minimized. If I'm correct real_loss can be used as it is right now\n",
    "def entropy_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    K = train_dataset.NUM_CLASSES\n",
    "    loss = torch.zeros(batch_size)\n",
    "    \n",
    "    # softmaxing\n",
    "    # e = torch.exp(D_out)\n",
    "    # s = torch.sum(e, dim=1)\n",
    "    # probabilities = e / s.view(BATCH_SIZE, 1)\n",
    "    \n",
    "    # Just regular normalization\n",
    "    \n",
    "    #probabilities = D_out / torch.sum(D_out, dim=1).view(batch_size, 1)\n",
    "    \n",
    "    #print(probabilities)\n",
    "            \n",
    "    for c in range(K):\n",
    "        # labels = torch.ones(batch_size) * c\n",
    "        # if train_on_gpu:\n",
    "        #     labels = labels.cuda()\n",
    "        \n",
    "        # c_loss = 1/K * torch.log(probabilities[:, c]) + (1 - 1/K) * torch.log(torch.ones(batch_size)-probabilities[:, c])         \n",
    "        c_loss = 1/K * torch.log(D_out[:, c]) + (1 - 1/K) * torch.log(torch.ones(batch_size)-D_out[:, c])         \n",
    "        \n",
    "        loss += c_loss\n",
    "    #print(loss)\n",
    "    return loss.sum() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%       \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-inf)\n",
      "tensor(-121.0368)\n",
      "tensor(-inf)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "test entropy loss\n",
    "''' \n",
    "D_out_min_entropy = torch.zeros(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "for i in range(BATCH_SIZE):\n",
    "    D_out_min_entropy[i][0] = 1\n",
    "D_out_random = torch.rand(BATCH_SIZE, train_dataset.NUM_CLASSES)\n",
    "\n",
    "D_out_max_entropy = torch.ones(BATCH_SIZE, train_dataset.NUM_CLASSES) \n",
    "\n",
    "print(entropy_loss(D_out_min_entropy))\n",
    "print(entropy_loss(D_out_random))\n",
    "print(entropy_loss(D_out_max_entropy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "#real_loss_criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_z = torch.randn(BATCH_SIZE, 100, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.SGD(D.parameters(), lr=lr)\n",
    "optimizerG = optim.SGD(G.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "FROM Udacity DCGAN implementation\n",
    "\"\"\"\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    print('GPU available for training. Models moved to GPU')\n",
    "else:\n",
    "    print('Training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JoseRonaldoPCF/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74797565 4.7875123 0.6987721 0.91719544\n",
      "0.057120737 -5.7929893 5.4345584\n",
      "- 5.4345584 0.91719544\n",
      "Epoch [    1/    5] | d_loss: 0.9172 | g_loss: 5.4346\n",
      "0.74780214 4.7870717 0.69778705 0.9154607\n",
      "0.057120737 -5.7930737 5.4332323\n",
      "- 5.4332323 0.9154607\n",
      "0.77858126 4.7872562 0.5946165 0.7958368\n",
      "0.057120737 -5.793006 5.273167\n",
      "- 5.273167 0.7958368\n",
      "0.8715407 4.7875113 0.49372518 0.72274184\n",
      "0.057120737 -5.793393 5.087617\n",
      "- 5.087617 0.72274184\n",
      "1.01192 4.7875733 0.3340667 0.48145854\n",
      "0.057120737 -5.793536 4.6971216\n",
      "- 4.6971216 0.48145854\n",
      "1.238984 4.787306 0.20066054 0.17411888\n",
      "0.057120737 -5.792698 4.1865573\n",
      "- 4.1865573 0.17411888\n",
      "1.613193 4.7876353 0.08545934 -0.41546273\n",
      "0.057120737 -5.792964 3.3332493\n",
      "- 3.3332493 -0.41546273\n",
      "2.137348 4.7873006 0.03346938 -1.0715919\n",
      "0.057120737 -5.793118 2.3959937\n",
      "- 2.3959937 -1.0715919\n",
      "2.8597589 4.7875566 0.009355629 -2.0550199\n",
      "0.057120737 -5.792378 1.1206007\n",
      "- 1.1206007 -2.0550199\n",
      "3.8075595 4.787935 0.0021973117 -3.217433\n",
      "0.057120737 -5.7930617 -0.32745886\n",
      "- -0.32745886 -3.217433\n",
      "4.701746 4.787401 0.0003570502 -4.8237123\n",
      "0.057120737 -5.792881 -2.144753\n",
      "- -2.144753 -4.8237123\n",
      "6.004123 4.7874355 5.50511e-05 -6.4488077\n",
      "0.057120737 -5.793003 -4.014246\n",
      "- -4.014246 -6.4488077\n",
      "7.6672893 4.787829 6.739099e-06 -8.304544\n",
      "0.057120737 -5.79267 -6.1149144\n",
      "- -6.1149144 -8.304544\n",
      "9.015154 4.787299 4.7311207e-07 -10.79906\n",
      "0.057120737 -5.7931027 -8.77083\n",
      "- -8.77083 -10.79906\n",
      "10.956293 4.787645 9.313226e-09 -14.531878\n",
      "0.057120737 -5.7925777 -12.699253\n",
      "- -12.699253 -14.531878\n",
      "12.926444 4.787426 0.0 -inf\n",
      "0.057120737 -5.793032 -inf\n",
      "- -inf -inf\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got nan at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/THNN/generic/BCECriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b4b0caa0f97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# 5) Forward pass real batch through D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mD_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_multi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.view(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0md_real_real_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 6.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0md_real_multi_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-11dde32c7da5>\u001b[0m in \u001b[0;36mreal_loss\u001b[0;34m(D_out, label, smooth)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Changed from BCEWithLogitsLoss, because I saw BCEWithLogitsLoss is for if you don't add the sigmoid loss yourself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2113\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got nan at /Users/distiller/project/conda/conda-bld/pytorch_1556653464916/work/aten/src/THNN/generic/BCECriterion.c:60"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "num_epochs = 5 #50\n",
    "print_every = 50\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for batch_i, (real_images, real_labels) in enumerate(train_dataloader, 0):\n",
    "        b_size = real_images.size(0)\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        # 3.\n",
    "        #z = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100)) \n",
    "        #z = torch.from_numpy(z).float()\n",
    "        z = torch.randn(b_size, 100, 1, 1, device=device)\n",
    "        #if train_on_gpu:\n",
    "        #    z = z.cuda()\n",
    "        \n",
    "        # 4) Generate fake image batch with G\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            real_images = real_images.cuda()\n",
    "        \n",
    "        # 5) Forward pass real batch through D\n",
    "        D_real, D_multi = D(real_images) #.view(-1)\n",
    "        d_real_real_loss = real_loss(D_real, label=1) \n",
    "        # 6.\n",
    "        d_real_multi_loss = multi_loss(D_multi, real_labels)\n",
    "        # 7.\n",
    "        D_fake, D_fake_entropy = D(fake_images)\n",
    "        d_fake_real_loss = real_loss(D_fake, label=0)\n",
    "        # 8.\n",
    "        g_fake_entropy_loss = entropy_loss(D_fake_entropy) ##\n",
    "        \n",
    "        # 9.\n",
    "        d_loss= torch.log(d_real_real_loss)+torch.log(d_real_multi_loss)+torch.log(d_fake_real_loss) \n",
    "        #torch.log(1-g_fake_real_loss), the 1- is not necessary because computed against label=0 now\n",
    "        print(d_real_real_loss.data.numpy(), d_real_multi_loss.data.numpy(), d_fake_real_loss.data.numpy(), d_loss.data.numpy())\n",
    "        \n",
    "        # 10.\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # 11.\n",
    "        g_loss=torch.log(d_fake_real_loss)-g_fake_entropy_loss\n",
    "        print(g_fake_real_loss.data.numpy(), g_fake_entropy_loss.data.numpy(), g_loss.data.numpy())\n",
    "        \n",
    "        # 12.\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        print(\"-\",g_loss.data.numpy(),d_loss.data.numpy())\n",
    "\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_i % print_every == 0:\n",
    "            # append discriminator loss and generator loss\n",
    "            G_losses.append(g_loss.item())\n",
    "            D_losses.append(d_loss.item())\n",
    "            \n",
    "            # print discriminator and generator loss\n",
    "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "    \n",
    "    ## AFTER EACH EPOCH##    \n",
    "    # generate and save sample, fake images\n",
    "    G.eval() # for generating samples\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "    img_z = G(fixed_z).detach().cpu()\n",
    "    img_list.append(make_grid(img_z, padding=2, normalize=True))\n",
    "    G.train() # back to training mode    \n",
    "    \n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(img_list, f)\n",
    "    \n",
    "torch.save(G, 'G.pt')\n",
    "torch.save(D, 'D.pt')\n",
    "\n",
    "\n",
    "### END -   FROM Udacity DCGAN implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
